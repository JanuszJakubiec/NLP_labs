{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37926408-f1a1-47d6-b0bd-69c8fcafa1a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from tqdm.notebook import tqdm\n",
    "from haystack.nodes import EmbeddingRetriever\n",
    "import math\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from scipy.special import softmax\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import (AutoModelForMaskedLM, AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, Trainer, TrainingArguments)\n",
    "\n",
    "# Tworzę document_store na potrzeby całego laboratorium. document_store, będzie służył do podstawowej części zadania. \n",
    "# Dokument document_store_for_another_retriever do opcjonalnej części, w której będę testował inny model językowy\n",
    "\n",
    "#document_store = FAISSDocumentStore(sql_url=\"sqlite:///my_faiss_index1.db\", similarity=\"cosine\",  embedding_dim=768)\n",
    "#document_store_for_another_retriever = FAISSDocumentStore(sql_url=\"sqlite:///my_faiss_index2.db\", similarity=\"cosine\", embedding_dim=768)\n",
    "document_store = FAISSDocumentStore.load(index_path=\"my_faiss_index1.faiss\")\n",
    "document_store_for_another_retriever = FAISSDocumentStore.load(index_path=\"my_faiss_index2.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645b7aa8-6151-46d6-a3af-e232908024b3",
   "metadata": {},
   "source": [
    "# Zadanie 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa36b8da-e873-4608-99d1-e7304102e2d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Deklaruję retriever, który będzie służył do updatowania embeddingów w document_store, a także do uzyskiwania z niego wartości.\n",
    "e5 = EmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "    embedding_model=\"intfloat/multilingual-e5-base\",\n",
    "    model_format=\"transformers\",\n",
    "    pooling_strategy=\"reduce_mean\",\n",
    "    top_k=5,\n",
    "    max_seq_len=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d77c34-06c5-4eef-bccc-28918dfecbd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Parsowanie danych\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "corpus_df = load_dataset(\"clarin-knext/fiqa-pl\", 'corpus')['corpus'].to_pandas()\n",
    "\n",
    "df = pd.DataFrame(corpus_df)\n",
    "data = []\n",
    "for index, row in df.iterrows():\n",
    "    # Dane odpowiedzi zapisuję w formie takiej, jaka jest polecana przez model e5.\n",
    "    data.append({\"content\": \"passage: \" + row[\"text\"], \"meta\": {\"id\": int(row[\"_id\"])}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3c51f0-9342-41f2-b3eb-8f23b81fe1c4",
   "metadata": {},
   "source": [
    "# Zadanie 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ee360aa-c0c5-4c53-b259-456e5af7fa57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Zapisywanie danych do dokuemntu\n",
    "#document_store.write_documents(data)\n",
    "\n",
    "# Przekształcanie dokumentu, zgodnie z e5\n",
    "#document_store.update_embeddings(e5)\n",
    "\n",
    "# Zapisywanie dokumentu, tak aby w przyszłości można go było wczytać\n",
    "#document_store.save(index_path=\"my_faiss_index1.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b84b0b2-b064-4452-8efa-84f43573009f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Przygotowanie datasetu z pytaniami\n",
    "dataset_questions = load_dataset(\"clarin-knext/fiqa-pl\",'queries')['queries'].to_pandas()\n",
    "dataset_questions[\"_id\"] = dataset_questions[\"_id\"].apply(lambda x: int(x))\n",
    "\n",
    "corpus_df[\"_id\"] = corpus_df[\"_id\"].apply(lambda x: int(x))\n",
    "qa_dataset_test = load_dataset('clarin-knext/fiqa-pl-qrels')['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66da2938-3b2a-46ee-889c-e3e74d28ae2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Funkcja która ma mi pomóc z niechcianymi paskami postępu. Gdy przetwarzam coś, często różne narzędzia wyświetlają swoje paski postępu, czego próbuję uniknąć\n",
    "# Czasami się udaje, czasami nie :(\n",
    "@contextmanager\n",
    "def silence_tqdm():\n",
    "    old_stdout = sys.stdout\n",
    "    old_stderr = sys.stderr\n",
    "    try:\n",
    "        with open(os.devnull, \"w\") as new_target:\n",
    "            sys.stdout = new_target\n",
    "            sys.stderr = new_target\n",
    "            yield new_target\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "        sys.stderr = \n",
    "\n",
    "# Funkcja do obliczania ndcg@5\n",
    "def count_ndcg5(ids_returned, ids_correct):\n",
    "    DCG = 0\n",
    "    for i in range(5):\n",
    "        if ids_returned[i] in ids_correct:\n",
    "            DCG += 1/math.log(i+2,2)\n",
    "    IDCG = 0\n",
    "    for i in range(min(len(ids_correct), 5)):\n",
    "        IDCG += 1/math.log(i+2, 2)\n",
    "    return DCG/IDCG\n",
    "\n",
    "# Funkcja, która dla konkretnego retrievera i pytania, zwraca ndcg@5. WAŻNE!!! parametr phrase_to_add_to_query opisuję w cellce poniżej.\n",
    "# Różne modele polecają dodawać przed pytanie różne stringi, na przykład dla e5, jest to \"query: \", to pole pozwala na wykorzystanie poniższej\n",
    "# funkcji niezależnie od modelu.\n",
    "def count_ndcg_for_question(retriever, question, proper_answers, phrase_to_add_to_query):\n",
    "    # Tu wykorzystuję wspomnianą na początku cell'ki funkcję silence_tqdm. Udało mi się nie wypisywać pasków postępu, które tworzył tqdm.\n",
    "    with silence_tqdm():\n",
    "        returned_answers = retriever.retrieve(query = phrase_to_add_to_query + question, top_k=5)\n",
    "        returned_answers = [document.meta[\"id\"] for document in returned_answers]\n",
    "        return count_ndcg5(returned_answers, proper_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefefd2c-14aa-4e80-828d-5b5a78d77a6d",
   "metadata": {},
   "source": [
    "# Zadanie 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fca980a5-ed12-41a3-b4a6-ace56603945f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36403aa942cb4d40948a8306ed4d9987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/648 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG%5 Score: 0.23468295908732414\n"
     ]
    }
   ],
   "source": [
    "unique_ids = qa_dataset_test[\"query-id\"].unique()\n",
    "result = []\n",
    "\n",
    "# Iteruję po unikalnych id pytań, w celu uzyskania ndcg@5.\n",
    "#iterations = 1\n",
    "iterations = len(unique_ids)\n",
    "for i in tqdm(range(iterations), desc=\"Processing\"):\n",
    "    id = unique_ids[i]\n",
    "    question = dataset_questions[dataset_questions['_id'] == id][\"text\"].to_list()[0]\n",
    "    # In material here https://github.com/deepset-ai/haystack/issues/5242 it is suggested to use \"question: \" before the question.\n",
    "    # However on the model documentation https://huggingface.co/intfloat/multilingual-e5-base it is said, that the 'query' string should be added.\n",
    "    # Results for the \"query: \" are better, than for the \"question: \".\n",
    "    result.append(count_ndcg_for_question(e5, question,  qa_dataset_test[qa_dataset_test[\"query-id\"] == id][\"corpus-id\"].values, \"query: \"))\n",
    "\n",
    "print(\"NDCG%5 Score:\", np.mean(np.array(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8f80b5-ea5b-40ca-83fd-13aa416f6e2f",
   "metadata": {},
   "source": [
    "# Zadanie 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da6b2ab-1cfa-4932-8b20-01ef42edb0ea",
   "metadata": {},
   "source": [
    "W mojej opinii wartość ndcg@5 uzyskana w tym ćwiczeniu, wynosząca 0.234 to bardzo dobry wynik. Porównując go z elasticsearch(\\~0.17) i modelem z labów 6(\\~0.14), wynik jest dużo wyższy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6146de81-6b4d-4f7e-9260-4eccc8664d3f",
   "metadata": {},
   "source": [
    "# Zadanie 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71dec6b4-5c69-4c33-98a8-e83b4f5bd700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Funkcje poniżej zostały skopiowane z labu 6. Jedyna różnica wystpuje w search_best_answers_model, gdzie nie jest wykonywane zapytanie do elasticsearch,\n",
    "# tylko do haystack.\n",
    "\n",
    "def get_trainer(source):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(source)\n",
    "    # Training args nie mają znaczenia. Abstrakcja trainer, jest używana jeydnie jako wygodny pojemnik na model, do wywołania funkcji\n",
    "    # trainer.predict(Datset).\n",
    "    training_args = TrainingArguments(output_dir=\"./results\", per_device_eval_batch_size=2)\n",
    "    trainer = Trainer(model,training_args)\n",
    "    return trainer\n",
    "\n",
    "def count_ndcg5(ids_returned, ids_correct):\n",
    "    DCG = 0\n",
    "    for i in range(5):\n",
    "        if ids_returned[i] in ids_correct:\n",
    "            DCG += 1/math.log(i+2,2)\n",
    "    IDCG = 0\n",
    "    for i in range(min(len(ids_correct), 5)):\n",
    "        IDCG += 1/math.log(i+2, 2)\n",
    "    return DCG/IDCG\n",
    "\n",
    "# Funkcja dokonuje zamiany pytania i zwróconych id z korpusu na dataset par pytanie odpowiedź, który będzie mógł zostać stokenizowany \n",
    "# i na którym bedzie mogła zostać przeprowadzona klasyfikacja przez model.\n",
    "def get_questions_from_corpus(question, corpus_df, answers_id):\n",
    "    result = []\n",
    "    for answer_id in answers_id:\n",
    "        value = corpus_df[corpus_df[\"_id\"] == answer_id][\"text\"].values\n",
    "        if len(value) == 0:\n",
    "            print(answer_id)\n",
    "        else:\n",
    "            result.append([question, value[0]])\n",
    "    return pd.DataFrame(result, columns=[\"query\", \"answer\"])\n",
    "\n",
    "# Funkcja do obliczenia NDCG dla konkretnego pytania, dla konkretnego modelu. Z haystack, uzyskuję SIZE_FOR_MODEL id zdań, które najlepiej\n",
    "# pasują do zadanego pytania. Następnie z uzyskanych pytań, tworzę pary z pytaniem, które potem zamienione na dataset huggingface, a\n",
    "# następnie są tokenizowane. Po tokenizacji dokonuję klasyfikacji modelem, a na koniec przetwarzam wyniki i zwracam obliczone ndcg@5.\n",
    "def search_best_answers_model(trainer, tokenizer, retriever, question, correct_answers, corpus_df, SIZE_FOR_MODEL):\n",
    "    tokenize_function = lambda example: tokenizer(example[\"query\"], example[\"answer\"], truncation=True, padding=\"max_length\")\n",
    "    with silence_tqdm():\n",
    "        #uzyskiwanie odpowiedzi z haystack\n",
    "        proposed_answers = retriever.retrieve(query = \"query: \" + question, top_k=SIZE_FOR_MODEL)\n",
    "        proposed_answers = [document.meta[\"id\"] for document in proposed_answers]\n",
    "        #dataset pandasa z kolumnami [query, answer], który zostanie potem zamieniony na dataset huggingface\n",
    "        pairs = get_questions_from_corpus(question, corpus_df, proposed_answers)\n",
    "        model_input = Dataset.from_pandas(pairs)\n",
    "        #tokenizacja\n",
    "        tokenized_model_input = model_input.map(tokenize_function)\n",
    "        #model klasyfikuje pary\n",
    "        results = trainer.predict(tokenized_model_input)\n",
    "    #liczę softmax z predykcji modelu, aby dostać prawdopodobieństwo należenia pary do pozytynej grupy.\n",
    "    results = softmax(results.predictions, axis = 1)\n",
    "    results_with_ids = list(zip(results, proposed_answers))\n",
    "    #sortuję wyniki, aby być w stanie otrzymać pierwszych 5 elementów\n",
    "    results_with_ids.sort(key=lambda x: x[0][1], reverse = True)\n",
    "    chosen_elements = [element[1] for element in results_with_ids][:5]\n",
    "    # na podstawie pierwszych 5 elementów, liczę ndcg i wynik zwracam\n",
    "    return count_ndcg5(chosen_elements, correct_answers)\n",
    "\n",
    "def perform_experiment_for_model(trainer, tokenizer, retriever, dataset_questions, corpus_df, qa_dataset_test, SIZE_FOR_MODEL):\n",
    "    results = []\n",
    "    ids = qa_dataset_test[\"query-id\"].unique()\n",
    "    #iterations = 10\n",
    "    iterations = len(ids)\n",
    "    for i in tqdm(range(iterations), desc=\"Processing\"):\n",
    "        id = ids[i]\n",
    "        question = dataset_questions[dataset_questions['_id'] == id][\"text\"].to_list()[0]\n",
    "        res = search_best_answers_model(trainer, tokenizer, retriever, question, qa_dataset_test[qa_dataset_test[\"query-id\"] == id][\"corpus-id\"].values, corpus_df, SIZE_FOR_MODEL)\n",
    "        results.append(res)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ead560-7416-4db3-86e4-ff26e2d154bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wczytuję model i obliczam NDCG@5\n",
    "with silence_tqdm():\n",
    "    trainer = get_trainer(\"work/checkpoint-10787\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "result = perform_experiment_for_model(trainer, tokenizer, e5, dataset_questions, corpus_df, qa_dataset_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d3d07e4-fe15-47d5-8767-7375e778cfc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028635361322172658\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.array(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98f1689-03be-484f-8172-ce7c2b74b2d6",
   "metadata": {},
   "source": [
    "Osiągnięte NDCG@5 jest bardzo kiepskie. Możliwe, że przez fakt, że przykłady do modelu były dobierane przez elasticsearch, model nauczył się w jakiś sposób rozróżniać dobre i słabe odpowiedzi na pytanie zwracane przez elasticearch. Możliwe, że te zwrócone przez haystack były lepsze niż te zwracane przez elasticsearch, przez co model pogubił się w ich rerankingu, co spowodowało bardzo słaby wynik."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07395f2c-b819-46f5-b4d0-131538915b9e",
   "metadata": {},
   "source": [
    "# Zadanie 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c96f4d85-e862-401b-b8de-a978880348b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deklaruję retriever roberta, parametry są używne trochę \"na czuja\", gdyż model nie jest tak dobrze opisany jak e5.\n",
    "roberta = EmbeddingRetriever(\n",
    "    document_store=document_store_for_another_retriever,\n",
    "    embedding_model=\"sdadas/mmlw-retrieval-roberta-base\",\n",
    "    model_format=\"transformers\",\n",
    "    pooling_strategy=\"reduce_mean\",\n",
    "    top_k=5,\n",
    "    max_seq_len=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82769c03-b70b-418a-83bd-8db81adda145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytuję dane. Dla tego modelu, content nie powinienn być poprzedzony żadną frazą.\n",
    "df = pd.DataFrame(corpus_df)\n",
    "data = []\n",
    "for index, row in df.iterrows():\n",
    "    data.append({\"content\": row[\"text\"], \"meta\": {\"id\": int(row[\"_id\"])}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1615920d-c56d-4bc3-a76a-557ffbc63d29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wykonuję operacje analogiczne, jak dla e5.\n",
    "\n",
    "#document_store_for_another_retriever.write_documents(data)\n",
    "#document_store_for_another_retriever.update_embeddings(roberta)\n",
    "#document_store_for_another_retriever.save(index_path=\"my_faiss_index2.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "484f24e1-1354-48f3-a501-f33e86bc415e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a918b8c4a447fcbe7795f244329762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/648 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG%5 Score: 0.21477659731945054\n"
     ]
    }
   ],
   "source": [
    "# W analogiczny sposób do e5 obliczam NDCG@5.\n",
    "\n",
    "unique_ids = qa_dataset_test[\"query-id\"].unique()\n",
    "result = []\n",
    "\n",
    "#iterations = 1\n",
    "iterations = len(unique_ids)\n",
    "for i in tqdm(range(iterations), desc=\"Processing\"):\n",
    "    id = unique_ids[i]\n",
    "    question = dataset_questions[dataset_questions['_id'] == id][\"text\"].to_list()[0]\n",
    "    # Model roberta poleca używać \"zapytanie: \", jako frazę przed pytaniem.\n",
    "    result.append(count_ndcg_for_question(roberta, question,  qa_dataset_test[qa_dataset_test[\"query-id\"] == id][\"corpus-id\"].values, \"zapytanie: \"))\n",
    "\n",
    "print(\"NDCG%5 Score:\", np.mean(np.array(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c3baf-6842-4d2e-acb5-3b8353132c98",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Which of the methods: lexical match (e.g. ElasticSearch) or dense representation works better?\n",
    "\n",
    "Odpowiedź na postawione pytanie z pozoru wydaje się bardzo prosta. Lepsze wyniki zostały bowiem osiągnięte dla dense representation, przez co wydawałoby się, że należy wskazać, że to dense representation działa lepiej niż lexical match taki jak elasticearch.\n",
    "\n",
    "Według mnie jednak nie możemy ograniczyć się jedynie do uzyskanego wyniku NDCG@5, gdyż wymienione narzędzia są dużo bardziej skomplikowane i dają większe możliwości niż jedynie uzyskanie parametru NDCG.\n",
    "\n",
    "Porównując możliwości obu rozwiazań, earto zauważyć, że na laboratorium korzystaliśmy z wielu możliwość które oferuje elasticsearch. Możemy go użyć chociażby do poprawy literówek(lab 3 wersja legacy). \n",
    "Użycie narzędzia haystack do dense representation ograniczyło się jedynie do question answeringu.\n",
    "Nie wątpię jednak, że poznane przez nas zastosowania to jedynie ułamek możliwości oferowanych przez narzędzia elasticserach i haystack.\n",
    "\n",
    "Porówując obie metody nie można pominąć aspektu wygody użycia, a w tym zdecydowanie przeaża dense representation. Najpewniej w pewnym stopniu zależy to od wybranego narzędzia, jednak do uruchomienia elasticserach konieczne było uruchomienie dockerowego kontenera. Dla wielu osób wiązało się to z problemami. Użycie dense representation było dużo wygodniejsze.\n",
    "\n",
    "Podsumowując bardzo trudno stwierdzić, które narzędzie działa lepiej. Do stwierdzenia tego faktu potrzebaby również bardziej skomplikowanej analizy, takiej jak na przykład analiza wykrzystywanych zasobów komputera.\n",
    "\n",
    "# Which of the methods is faster?\n",
    "\n",
    "Czas przetwarzania pytań dla dense representation był wielokrotnie wyższy niż ten wymagany dla elasticsearch. Operacja zapisania danych i czas potrzebny na ich przekształcenie dla dense representation to około 40 minut. Wczytanie danych do elasticsearch to około 2 minuty, dla danych tego rozmiaru. Wyniki dense representation zależą w dużym stopniu od wybranego modelu językowego.\n",
    "\n",
    "Czas potrzebny do wykonania zapytań był porównywalny i bardzo szybki.\n",
    "\n",
    "# Try to determine the other pros and cons of using lexical search and dense document retrieval models.\n",
    "\n",
    "Zarówno lexical serach, jak i dense document retrieval models dają nam dostęp do bardzo potężnego narzędzia, jakim jest znajdywanie odpowiedzi na zadane pytanie. Jest to bardzo złożony problem, który bardzo trudno rozwiązać w inny sposób niż przy ich użyciu.\n",
    "\n",
    "Jedym z plusów obu rozwiązań jest szybkość odpowiadania na zapytanie. Zarówno lexical search, jaki document retrieval models wykonują zapytania dużo szybciej niż stworzony przeze mnie model z laboratorium 6. Sprawdzą się zatem wszędzie tam, gdzie liczy się czas odpowiedzi na pytanie. Model z laboratorium 6, jest w praktyce nieużywalny do przeszukiwania bazy 65000 pytań, gdyż czas przetwarzania byłby zbyt duży. W sytuacji gdy chcemy przetworzyć dużą bazę pytań, musimy wykorzystać wymienione w pytaniu metody.\n",
    "\n",
    "Dużą wadą narzędzia lexical search, jest to, że nie bierze on pod uwagę kontekstu w którym piszemy zdanie, jedynie zwraca uwagę na występujące słowa. Dzięki temu jednak wczytywanie bazy, a także korzystanie z niej jest dużo szybsze niż w przypadku document retrieva. i narzędzia haystack."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

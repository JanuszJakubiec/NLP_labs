{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6397fa1-7e08-49aa-a6d1-7f230e0b7e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "import evaluate\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (AutoModelForMaskedLM, AutoModelForQuestionAnswering, AutoTokenizer, DataCollatorWithPadding, Trainer, TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea5a6b9b-c7c9-4315-a27f-59e60b2f7317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytuję dane poquad\n",
    "with open(\"poquad-train.json\", 'r') as file:\n",
    "    train_data = json.load(file)['data']\n",
    "with open(\"poquad-dev.json\", 'r') as file:\n",
    "    val_data = json.load(file)['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4083ada2-b680-4c68-909b-9d53562dbd4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tworzę dataset z danych poquad\n",
    "def proccess_the_data(data):\n",
    "    input_data = []\n",
    "    [\n",
    "        [\n",
    "            [\n",
    "                input_data.append(\n",
    "                    [paragraph_element[\"context\"], question[\"question\"], question[\"answers\"][0]]\n",
    "                ) if \"answers\" in question.keys() else input_data.append(\n",
    "                    # czasami w danych nie ma pola\"answers\", jest \"plausible_answers\", wtedy wczytuję plasuible_answers\n",
    "                    [paragraph_element[\"context\"], question[\"question\"], question[\"plausible_answers\"][0]]\n",
    "                ) \n",
    "                for question in paragraph_element[\"qas\"]\n",
    "            ] \n",
    "            for paragraph_element in element[\"paragraphs\"]\n",
    "        ] \n",
    "        for element in data\n",
    "    ]\n",
    "    \n",
    "    return [[str(id), element[0], element[1], element[2][\"answer_start\"], element[2][\"answer_end\"], element[2][\"text\"]] for id, element in enumerate(input_data)]\n",
    "\n",
    "# Tworzę dataset testowy w wersji raw, czyli niestokenizowanej\n",
    "train_dataset_dataframe = pd.DataFrame(proccess_the_data(train_data), columns=[\"id\", \"context\", \"question\", \"answer_start\", \"answer_end\", \"answer_text\"])\n",
    "train_dataset_raw = Dataset.from_pandas(train_dataset_dataframe)\n",
    "\n",
    "# Tworzę dataset validacyjny w wersji raw, czyli niestokenizowanej\n",
    "val_dataset_dataframe = pd.DataFrame(proccess_the_data(val_data), columns=[\"id\", \"context\", \"question\", \"answer_start\", \"answer_end\", \"answer_text\"])\n",
    "val_dataset_raw = Dataset.from_pandas(val_dataset_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ccda9da-e0d7-4439-a01f-0245682c5481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja dostosowująca dataset do postaci, która może zostać użyta do trenowania modelu\n",
    "def preprocess_data(data, tokenizer, stride, max_length):\n",
    "    # dane zostają stokenizowane\n",
    "    tokenized_data = tokenizer(\n",
    "        data[\"question\"],\n",
    "        data[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    offset_mapping = tokenized_data[\"offset_mapping\"]\n",
    "    sample_map = tokenized_data.pop(\"overflow_to_sample_mapping\")\n",
    "    # model w trakcie uczenia będzie zwracał początek i koniec fragmentu kontekstu, który odpowiada na zadane pytanie\n",
    "    # Poniżej przygotowuję listy, w których dla każdego stokeniozowanego przykładu będę zapisywał od którego tokenu się zaczyna\n",
    "    # i na którym tokenie kończy się odpowiedź na zadane pytanie\n",
    "    \n",
    "    # Dodatkową trudnością jest sytuacjia, gdy kontekst i pytanie mają długość większą niż 300 tokenów, zostaną wtedy podzielone na 2 przypadki uczące,\n",
    "    # może przez to dojść do sytuacji, gdy we fragmencie kontekstu nie znajdzie się odpowiedź na zadane pytanie. W takiej sytuacji jako zarówno początek\n",
    "    # jak i koniec odpowiedzi zostanie ustawiona wartość 0.\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # iteruję po przykładach uczących wyznaczając początek i koniec odpowiedzi.\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        start_char = data[\"answer_start\"][sample_idx]\n",
    "        end_char = data[\"answer_end\"][sample_idx]\n",
    "        sequence_ids = tokenized_data.sequence_ids(i)\n",
    "\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    tokenized_data[\"start_positions\"] = start_positions\n",
    "    tokenized_data[\"end_positions\"] = end_positions\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a774ee3-0d3e-4454-8089-14d5a941bb26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f97f6d180042ee98ecad0468b56f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(65556, 56618)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Przekształcam dartaset treningowy na huggingface'owy dataset, który nadaje się do wykorzystania w treningu\n",
    "stride = 70\n",
    "max_length = 300\n",
    "model_checkpoint = \"allegro/plt5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "preproccess_lambda = lambda x: preprocess_data(x, tokenizer, stride, max_length)\n",
    "\n",
    "train_dataset = train_dataset_raw.map(\n",
    "    preproccess_lambda,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset_raw.column_names,\n",
    ")\n",
    "\n",
    "len(train_dataset), len(train_dataset_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b852906e-e06d-40d7-a304-604a6a1cb74f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ec24518e09464ab544e70e24afcda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7060 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(8164, 7060)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Przekształcam dartaset walidacyjny na huggingface'owy dataset, który nadaje się do wykorzystania w treningu\n",
    "# oraz w testach modelu\n",
    "stride = 70\n",
    "max_length = 300\n",
    "\n",
    "preproccess_lambda = lambda x: preprocess_data(x, tokenizer, stride, max_length)\n",
    "\n",
    "val_dataset = val_dataset_raw.map(\n",
    "    preproccess_lambda,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset_raw.column_names,\n",
    ")\n",
    "\n",
    "len(val_dataset), len(val_dataset_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7617ac9b-6180-4e46-953d-a851787f3346",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Poniżej hiperparametry, które zostały wykorzystane do trenowania modelu. Trening został przeprowadzony na 3 epokach, zgodnie\n",
    "# z wymaganiami wyszczególnionymi w treści zadania.\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#checkpoint = \"allegro/plt5-base\"\n",
    "checkpoint = \"finetuning-test-full/checkpoint-4097\"\n",
    "#model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"finetuning-test-full\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    warmup_steps=600,\n",
    "    save_steps = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80bf585b-4f5e-4f16-bc8b-f07777dd2a5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# poniżej trainer, któy dokonał treningu\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "\n",
    "# poniższy kod pozwolił mi na wznowienie treningu z checkpointa. Dzięki temu byłem w stanie wykonać cały trening 3 epok, który łącznie zajął na \n",
    "# moim sprzęcie ponad 12 godzin.\n",
    "# trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8655d960-9db1-4ae1-bccb-384e5871d6a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Poniżej kod, który na podstawie datasetu \"simple legal questions\" tworzy dataset, który może zostać wykorzystany do\n",
    "# przetestowania wytrenowanego modelu. Cała operacja jest dość skomplikowna i wymaga czytania z 4 plików, a następnie\n",
    "# połączenia danych.\n",
    "\n",
    "# W poniższym pliku znajdują się id pytań, oraz odpowiedzi na nie.\n",
    "with open(\"answers.jl\", 'r') as file:\n",
    "    data_answers_test_answers = [json.loads(line) for line in file]\n",
    "\n",
    "data_answers_test_answers = pd.DataFrame(data_answers_test_answers)\n",
    "data_answers_test_answers = data_answers_test_answers[data_answers_test_answers[\"score\"] == \"1\"]\n",
    "\n",
    "# W poniższym pliku znajdują się teksty, będące kontekstem\n",
    "with open(\"passages.jl\", 'r') as file:\n",
    "    data_answers_test_passages = [json.loads(line) for line in file]\n",
    "data_answers_test_passages = pd.DataFrame(data_answers_test_passages)\n",
    "\n",
    "# W ponizszym pliku znajduje się ralacja pomiędzy id pytania, a kontekstem, do którego zostaje zadane pytanie\n",
    "with open(\"relevant.jl\", 'r') as file:\n",
    "    data_answers_test_relevant = [json.loads(line) for line in file]\n",
    "data_answers_test_relevant = pd.DataFrame(data_answers_test_relevant)\n",
    "\n",
    "# W poniższym pliku znajdują treści pytań w relacji do ich id\n",
    "with open(\"questions.jl\", 'r') as file:\n",
    "    data_answers_test_questions = [json.loads(line) for line in file]\n",
    "data_answers_test_questions = pd.DataFrame(data_answers_test_questions)\n",
    "\n",
    "\n",
    "# poniżej pola które będzie zawierał gotowy dataset\n",
    "#[id, question, context, answer_text]\n",
    "\n",
    "data_test_dataframe = []\n",
    "\n",
    "# Iteruję po id pytania, na które znajduje się odpowiedź, a następnie \n",
    "for question_id in data_answers_test_answers[\"question-id\"].unique():\n",
    "    context_id = data_answers_test_relevant[data_answers_test_relevant[\"question-id\"] == question_id][\"passage-id\"].values[0]\n",
    "    context = data_answers_test_passages[data_answers_test_passages[\"_id\"] == context_id][\"text\"].values[0]\n",
    "    question = data_answers_test_questions[data_answers_test_questions[\"_id\"] == question_id][\"text\"].values[0]\n",
    "    answer_text = data_answers_test_answers[data_answers_test_answers[\"question-id\"] == question_id][\"answer\"].values[0]\n",
    "    data_test_dataframe.append([question_id, question, context, answer_text])\n",
    "\n",
    "data_test_dataframe = pd.DataFrame(data_test_dataframe, columns=[\"id\", \"question\",\"context\", \"answer_text\"])\n",
    "data_test_raw = Dataset.from_pandas(data_test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b38464f-0256-453d-896c-ed002255835e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Funkcja wczytująca model, do abstrakcji jaką jest trainer.\n",
    "def get_trainer(source):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(source)\n",
    "    # Training args nie mają znaczenia. Abstrakcja trainer, jest używana jeydnie jako wygodny pojemnik na model, do wywołania funkcji\n",
    "    # trainer.predict(Datset).\n",
    "    training_args = TrainingArguments(output_dir=\"./results\", per_device_eval_batch_size=1)\n",
    "    trainer = Trainer(model,training_args)\n",
    "    return trainer\n",
    "\n",
    "#source = \"apohllo/plt5-base-poquad\"\n",
    "source = \"Checkpoint-wtorek\"\n",
    "\n",
    "trainer2 = get_trainer(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c3c4e88-1791-4019-901c-769dbc15746c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8f1d1b57704bccb15de9bfd4a18ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(542, 484)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Funkcja dostosowująca dataset testowy, w taki sposób, aby był używalny przez model.\n",
    "def preprocess_input(data, stride, max_length):\n",
    "    tokenized_data = tokenizer(\n",
    "        data[\"question\"],\n",
    "        data[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    return tokenized_data\n",
    "\n",
    "stride = 70\n",
    "max_length = 300\n",
    "\n",
    "process_input_lambda = lambda x: preprocess_input(x, stride, max_length)\n",
    "\n",
    "test_dataset = data_test_raw.map(\n",
    "    process_input_lambda,\n",
    "    batched=True,\n",
    "    remove_columns=data_test_raw.column_names,\n",
    ")\n",
    "\n",
    "len(test_dataset), len(data_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a96f51e-2804-40e9-8016-feef6c3ea85d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Uzyskuję wyniki dla walidacyjnego datasetu\n",
    "results = trainer2.predict(test_dataset)\n",
    "predictions, _, _ = results\n",
    "start_logits, end_logits, _ = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4121da8-a53a-402d-a95d-2d7b26eda0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja służąca do obliczania metryk exact match i f1 score.\n",
    "def compute_metrics(start_logits, end_logits, tokenized_data, references, n_best, max_answer_length):\n",
    "    predicted_answers = []\n",
    "    tokenized_words = []\n",
    "    ids = []\n",
    "    \n",
    "    # Iteruję po datasecie, dla każdego z przykładów zostanie stworzony słownik zawierający: id przykładu\n",
    "    # oraz odpowiedź na pytanie, najbardziej prawdopodobną według modelu.\n",
    "    for index, example in tqdm(enumerate(references)):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "        \n",
    "        start_logit = start_logits[index]\n",
    "        end_logit = end_logits[index]\n",
    "        offsets = tokenized_data[index][\"offset_mapping\"]\n",
    "        tokenized_words.append([(context[start : end]) for start, end in list(offsets)])\n",
    "\n",
    "        # uzyskuję n_best id początku odpowiedzi oraz n_best id końca odpowiedzi.\n",
    "        # uzyskiwane są, poprzez wybranie 20 id, pod którymi znajdują się największe wartości przewidywane przez model.\n",
    "        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "        \n",
    "        # Iteruję przez wszystkie wybrane początki odpowiedzi, oraz wszystkie końce odpowiedzi.\n",
    "        # Wyciągam z kontekstu fragment od znaku start_index, aż do end_index i następnie dodaję do listy \"answers\"\n",
    "        for start_index in start_indexes:\n",
    "            for end_index in end_indexes:\n",
    "                # W lisćie offsets znajdują się początki każdego z tokenów.\n",
    "                if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                    continue\n",
    "                # max_answer_length to podana jako parametr maksymalna liczba tokenów w odpowiedzi, aby była ona rozpatrywana\n",
    "                if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                    continue\n",
    "                if start_index == 0 and end_index == 0:\n",
    "                    continue\n",
    "\n",
    "                answer = {\n",
    "                    \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                    \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    \"ids\": [start_index, end_index]\n",
    "                }\n",
    "                answers.append(answer)\n",
    "        # dla konkretnego elementu, wybierany jest najbardziej prawdopodobna predykcja\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "            ids.append(best_answer[\"ids\"])\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": {\"text\": [ex[\"answer_text\"]], \"answer_start\": [1]}} for ex in references]\n",
    "    metric = evaluate.load(\"squad\")\n",
    "    return (predicted_answers, theoretical_answers, metric.compute(predictions=predicted_answers, references=theoretical_answers), tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e8e1a19-a778-4290-bb12-9be9f554b3d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4bacf99020e4d86b435396dd8ae178b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0.0, 'f1': 10.683034201763867}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = compute_metrics(start_logits, end_logits, test_dataset, data_test_raw, 20, 60)\n",
    "res[2]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6fd885df-65f3-491e-906d-c68fe8cb1e0a",
   "metadata": {},
   "source": [
    "Osiągnięte wyniki są bardzo złe. Warto zatem przeanalizować odpowiedzi wygenerowane przez model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37d13335-3ed9-4730-9ce5-f37f751711d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poprnawna odpowiedź: Tak, podlega karze aresztu wojskowego albo pozbawienia wolności do lat 3.\n",
      "Odpowiedź zwrócona przez model: Art. 345. § 1. Żołnierz, który dopuszcza się czynnej napaści na przełożonego, podlega karze aresztu wojskowego albo pozbawienia wolności do lat 3.\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Poprnawna odpowiedź: Komisja przetargowa składa się z co najmniej trzech osób.\n",
      "Odpowiedź zwrócona przez model: Art. 21. 1. Członków komisji przetargowej powołuje i odwołuje kierownik zamawiającego. 2. Komisja przetargowa\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Poprnawna odpowiedź: Komandytariusz odpowiada za zobowiązania spółki wobec jej wierzycieli tylko do wysokości sumy komandytowej.\n",
      "Odpowiedź zwrócona przez model: Art. 111. Komandyt\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Poprnawna odpowiedź: Wartość rzeczowych składników majątku obrotowego, które utraciły swoje cechy użytkowe lub przydatność ustala się nie później niż na dzień bilansowy w cenach sprzedaży netto możliwych do uzyskania\n",
      "Odpowiedź zwrócona przez model: Art. 35. 1. Wartość rzeczowych składników majątku obrotowego, które utraciły swoje cechy użytkowe lub przydatność, oraz odpadów ustala się nie później niż na dzień bilansowy\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Poprnawna odpowiedź: Podlega karze pieniężnej do wysokości 1 000 000 złotych\n",
      "Odpowiedź zwrócona przez model: Art. 74. 1. Armator, który wykonuje rybołówstwo\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Poprnawna odpowiedź: Według zasady wzajemności z akcyzy zwalnia się instytucje Wspólnot Europejskich, organizacje międzynarodowe, przedstawicielstwa dyplomatyczne, urzędy konsularne oraz członków personelu tych przedstawicielstw i urzędów, a także innych osób zrównanych z nimi na podstawie ustaw, umów lub zwyczajów międzynarodowych, jeżeli nie są obywatelami polskimi i nie mają stałego miejsca pobytu na terytorium kraju.\n",
      "Odpowiedź zwrócona przez model: Art. 31. 1. Zwalnia się od akcyzy, jeżeli wynika to z porozumień międzynarodowych, lub zasady wzajemności, czynności podlegające opodatkowaniu akcyzą wobec instytucji Wspólnot Europejskich\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Poprnawna odpowiedź: Nie, żołnierze przed przystąpieniem do wykonania czynności służbowej są obowiązani przedstawić się.\n",
      "Odpowiedź zwrócona przez model: Art. 12. 1. Żołnierze Żandarmerii Wojskowej przed przystąpieniem do wykonania czynności służbowej są obowiązani przedstawić się, podając stopień wojskowy oraz imię i nazwisko, a ponadto na żądanie osoby, której czynność ta dotyczy, są obowiązani okazać legitymację żołnierza Żandarmerii Wojskowej w sposób umożliwiający odczytanie oraz zanotowanie serii i numeru legitymacji, a także danych osobowych żołnierza. 2. Przepis art. 11 ust. 2 stosuje się odpowiednio.\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Poprnawna odpowiedź: Miasto na prawach powiatu sporządza jeden budżet.\n",
      "Odpowiedź zwrócona przez model: Art.\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Poprnawna odpowiedź: Wykonując czynności operacyjno-rozpoznawcze, żołnierze Żandarmerii Wojskowej mogą posługiwać się dokumentami, które uniemożliwiają ustalenie danych.\n",
      "Odpowiedź zwrócona przez model: Art. 40. 1. Żandarmeria Wojskowa, wykonując czynności operacyjno-rozpoznawcze, zapewnia ochronę środków, form i metod ich wykonywania, zgromadzonych informacji oraz własnych obiektów i danych identyfikacyjnych pozwalających na ustalenie tożsamości żołnierzy Żandarmerii Wojsk\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Poprnawna odpowiedź: W celu znalezienia i zatrzymania przedmiotów podlegających oględzinom lub mogących stanowić dowód rzeczowy, jeżeli istnieją uzasadnione podstawy do przypuszczenia, że przedmioty te lub dowody tam się znajdują.\n",
      "Odpowiedź zwrócona przez model: Art. 44. §1. W celu zna\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    print(\"Poprnawna odpowiedź:\", res[1][i][\"answers\"][\"text\"][0])\n",
    "    print(\"Odpowiedź zwrócona przez model:\", res[0][i][\"prediction_text\"])\n",
    "    print(\"-----------------------------------------------------------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f12484-17cd-49c2-9123-100f6c68bb33",
   "metadata": {},
   "source": [
    "Jak widać w przykładach wypisanych powyżej, zazwyczaj początek kontekstu stanowi zarazem początek odpowiedzi. Przeglądając odpowiedzi można zauważyć, że niektóre zawierają nieco sensu. Tak jest chociażby w odpowiedzi 1, która zawiera odpowiedź spójną ze wzorcową odpowiedzią. Warto tutaj również zaznaczyć, że dataset który został wykorzystany do odpowiedzi na pytania jest stworzony przez studentów i bardzo często odpowiedzi zostały przygotowane dla podejścia AQA, natomiast mój model został wytrenowany i jest przewidziany do korzystania z podejścia EQA, przez co wyniki mogą być gorsze niż są w rzeczywistości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57560616-7d5a-4835-b5c4-044abc627434",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8b5fee27cc404c934800a827d77e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aecb452f5cdd415da31487a68668fd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Poniższy kod jest niezbędny do uzyskania wyników dla datasetu walidacyjnego. Wykonanie za pomocą\n",
    "# abstrakcji jaką jest trainer, powoduje błąd związany z niewystarczającą ilością pamięci.\n",
    "# Problem nie wstępuje, gdy wywołuję funkcję ewaluującą bezpośrednio na modelu.\n",
    "val_dataset.set_format(\"torch\")\n",
    "eval_set_for_model = val_dataset.remove_columns([\"offset_mapping\"])\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\n",
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(\"Checkpoint-wtorek\").to(\n",
    "    device\n",
    ")\n",
    "\n",
    "mini_batches = []\n",
    "for i in tqdm(range(len(batch['input_ids']))):\n",
    "    # Create a mini-batch for each element\n",
    "    mini_batch = {k: v[i].unsqueeze(0).to(device) for k, v in batch.items()}\n",
    "    mini_batches.append(mini_batch)\n",
    "\n",
    "start_logits_arr = []\n",
    "end_logits_arr = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for element_id in tqdm(range(len(mini_batches))):\n",
    "        element = mini_batches[element_id]\n",
    "        outputs = trained_model(**element)\n",
    "        start_logits = outputs.start_logits.cpu().numpy()\n",
    "        end_logits = outputs.end_logits.cpu().numpy()\n",
    "        start_logits_arr.append(start_logits[0])\n",
    "        end_logits_arr.append(end_logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bddf691-ba50-45e3-a914-2bccffecced4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ddaca35d844f329f7c068f1c351b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res2 = compute_metrics(start_logits_arr, end_logits_arr, val_dataset, val_dataset_raw, 20, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccf11285-3aed-47f8-8e47-365a76438b43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 0.014164305949008499, 'f1': 8.3200053715511}\n"
     ]
    }
   ],
   "source": [
    "print(res2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d966d25-490f-4534-8add-76cebcecfacd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poprnawna odpowiedź: kompilację poglądów różnych rabinów na określony temat\n",
      "Odpowiedź zwrócona przez model: Pisma rabiniczne – w tym Miszna – stanowią kompilację poglądów różnych rabinów na określony temat. Zgodnie z wierzeniami judaizmu Mojżesz otrzymał od Boga całą Torę\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Poprnawna odpowiedź: dwóch\n",
      "Odpowiedź zwrócona przez model: Pisma rabiniczne – w tym Miszna – stanowią kompilację poglądów różnych rabinów na określony temat. Zgodnie z wierzeniami judaizmu\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Poprnawna odpowiedź: pisanej, a drugą część w formie ustnej\n",
      "Odpowiedź zwrócona przez model: Pisma rabiniczne – w tym Miszna – stanowią kompilację poglądów różnych rabinów na określony temat. Zgodnie z wierzeniami judaizmu\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Poprnawna odpowiedź: ustna\n",
      "Odpowiedź zwrócona przez model: Pisma rabiniczne – w tym Miszna – stanowią kompilację poglądów różnych rabinów na określony temat. Zgodnie z wierzeniami judaizmu\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Poprnawna odpowiedź: Boga\n",
      "Odpowiedź zwrócona przez model: Pisma rabiniczne – w tym Miszna – stanowią kompilację poglądów różnych rabinów na określony temat. Zgodnie z wierzeniami judaizmu\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Poprnawna odpowiedź: 280 strzelców, kilkuset chłopów kosynierów i 60 kawalerzystów\n",
      "Odpowiedź zwrócona przez model: Sformowany przez nią oddział partyzancki liczył 280 strzelców, kilkuset chłopów kosynierów i 60 kawalerz\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Poprnawna odpowiedź: 29 marca 1831\n",
      "Odpowiedź zwrócona przez model: Sformowany przez nią oddział partyzancki liczył 280 strzelców, kilkuset chłopów kosynierów i 60 kawalerzystów. Wraz z oddziałem rozpoczęła marsz w kierunku Dyneburga, po drodze do oddziału dołączali wciąż ochotnicy. 29 marca 1831 wkroczyła do Dusiatów, gdzie pod rozwiniętą flagą Polski zachęcała okoliczną ludność do przyłączenia się do powstania. 30 marca 1831 wraz ze swym hufcem zajęła stację Daugiele. 2 kwietnia jej oddział stoczył zwycięską potyczkę i zniósł kompanię piechoty rosyjskiej pod Ucianami. 4 kwietnia uderzyła na jedną z kolumn korpusu gen. Schirmana, która zmierzała w rejon koncentracji głównych sił rosyjskich w Dyneburgu. Oddział Emilii Plater opanował Jeziorosy, gdzie zdążono wpisać do akt grodzkich akt powstania. Zaskoczenie Dyneburga nie powiodło się powstańcom, młodzi zrewoltowani podchorążowie zostali wysłani do obozu Dybicza. Wobec pogarszającego się położenia militarnego, Emilia zrezygnowała ostatecznie z ataku na Dyneburg. Emilia dzieliła z podkomendnymi wszystkie trudy walki partyzanckiej, zyskała sobie wkrótce ich miłość i uznanie, chociaż w świecie salonów nie szczędzono jej złośliwych uwag i zarzutów niemoralności\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Poprnawna odpowiedź: 4 kwietnia\n",
      "Odpowiedź zwrócona przez model: Sformowany przez nią oddział partyzancki liczył 280 strzelców, kilkuset chłopów kosynierów i 60 kawalerzystów. Wraz z oddziałem rozpoczęła marsz w kierunku Dyneburga\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Poprnawna odpowiedź: zrezygnowała ostatecznie z ataku na Dyneburg\n",
      "Odpowiedź zwrócona przez model: Sformowany przez nią oddział partyzancki liczy\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Poprnawna odpowiedź: pod Ucianą i Oniksztami\n",
      "Odpowiedź zwrócona przez model: Sformowany przez nią oddział partyzancki liczył 280 strzelców, kilkuset chłopów kosynierów i 60 kawalerzystów. Wraz z oddziałem rozpoczęła marsz w kierunku Dyneburga\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    print(\"Poprnawna odpowiedź:\", res2[1][i][\"answers\"][\"text\"][0])\n",
    "    print(\"Odpowiedź zwrócona przez model:\", res2[0][i][\"prediction_text\"])\n",
    "    print(\"-----------------------------------------------------------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd3d6c5-0e1c-4c40-a520-3b552303fa7a",
   "metadata": {},
   "source": [
    "Dla datasetu walidacyjnego obserwacje są bardzo podobne, jak dla datasetu testowego. Tutaj warto zauważyć, że np. dla przykładów 1-4 odpowiedzi na pytania są dokładnie takie same, pomimo tego, że treść pytania jest inna. Warto zwrócić uwagę na poprawną odpowiedź dla pytania 5, gdzie poprawna odpowiedź to: **280 strzelców, kilkuset chłopów kosynierów i 60 kawalerzystów**, natomiast model udzielił: **Sformowany przez nią oddział partyzancki liczył 280 strzelców, kilkuset chłopów kosynierów i 60 kawalerz** co jest bardzo podobne. Według mnie model poradził sobie lepiej na datasecie walidacyjnym niż na datasecie testowym. Jest to zrozumiałe, gdyż pytania i kontekst są podobne do danych testowych. Natomiast dane w zbiorze testowym znacząco się od nich różnią."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aca03d2-20eb-4567-beec-021907cbda74",
   "metadata": {},
   "source": [
    "## Does the performance on the validation dataset reflects the performance on your test set?\n",
    "\n",
    "Jak można zobaczyć po metrykach, ich wyniki są bardzo podobne dla obu datasetów. Warto jednak pamiętać, że dataset testowy jest dużo mniejszy niż dataset walidacyjny, więc bardzo ciężko porównać te oba wyniki.\n",
    "\n",
    "Kolejnym problemem jest wspomniany wcześniej fakt, że dataset testowy został stworzony przez studentów i jest bardziej datasetem dla problemu AQA, niż wykorzystany w tym laboratorium EQA.\n",
    "\n",
    "## What are the outcomes of the model on your test questions? Are they satisfying? If not, what might be the reason for that?\n",
    "\n",
    "Wyniki nie są zadowalające. Niestety, pomimo starań nie dysponuję sprzętem na tyle silnym, aby być w stanie wytrenować model wystarczająco dobrze, aby był w stanie odpowiadać na zadane pytania w zadowalający sposób. Przy większych zasobach na pewno możliwe byłoby stworzenie modelu, który w zadowalający sposób odpowiedziałby na zadane pytania.\n",
    "\n",
    "## Why extractive question answering is not well suited for inflectional languages?\n",
    "\n",
    "Wydaje mi się, że największym problemem jest różnica w formie słów, które są wymagane przez sposób zadania pytania, w stosunku do tego w jakiej formie słowa występują w kontekście. Model może mieć przez to problem ze znalezieniem odpowiedniego fragmentu kontekstu, który odpowiada na zadane pytanie, a nawet jeżeli zostanie on poprawnie znaleziony, to korzystanie z takiej odpowiedzi może być niekomfortowe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
